{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Name   : Rupesh Garsondiya\n",
        "github : @Rupeshgarsondiya\n",
        "Topic  : NN module and torch.optim in torch (Deep-Learning with PyTorch)\n",
        "'''"
      ],
      "metadata": {
        "id": "Y7Bfscs0m1HS",
        "outputId": "1b1459d1-2807-4a13-f7dd-306fc41c3a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nName   : Rupesh Garsondiya\\ngithub : @Rupeshgarsondiya\\nTopic  : NN module and torch.optim in torch (Deep-Learning with PyTorch)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN module of PyTorch\n",
        "\n",
        "- Core torch library it's  provide wide array of classes and functions designed to help devloper build neural network efficiently and effectively\n",
        "\n",
        "**Key components of torch.nn :**\n",
        "\n",
        "1. Modules (Layers)\n",
        "\n",
        "   Examples: nn.Linear, nn.Conv2d, nn.LSTM (recurrent layer)\n",
        "\n",
        "2. Activation Functions\n",
        "   \n",
        "   Examples: nn.Sigmoid, nn.ReLU, nn.Tanh\n",
        "\n",
        "3. Loss Functions\n",
        "\n",
        "   Examples: nn.CrossEntropyLoss, nn.MSELoss\n",
        "\n",
        "4. Container Modules\n",
        "\n",
        "   Example: nn.Sequential (A sequential container to stack layers in order)\n",
        "\n",
        "5. Regularization and Dropout\n",
        "   \n",
        "   Examples: nn.BatchNorm2d, nn.Dropout\n",
        "\n",
        "\n",
        "\n",
        "**This is totally practical 😀, so let's understand it with code.**"
      ],
      "metadata": {
        "id": "9rh2KntZoDyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create model class\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "IjZiD_ZznXML"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sci-kit learn in build dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "print(\"X shape : \",X.shape)\n",
        "print(\"y shape : \",y.shape)\n",
        "\n",
        "\n",
        "X = torch.FloatTensor(X)\n",
        "y = torch.FloatTensor(y)\n",
        "\n",
        "\n",
        "print(\" Type X : \",type(X))\n",
        "print(\" Type y : \",type(y))"
      ],
      "metadata": {
        "id": "1D7gM3ZcxJ-j",
        "outputId": "370813c3-05b4-4085-a747-b8850314bdcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape :  (569, 30)\n",
            "y shape :  (569,)\n",
            " Type X :  <class 'torch.Tensor'>\n",
            " Type y :  <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module) :\n",
        "\n",
        "    def __init__(self,num_feature) :\n",
        "\n",
        "        super().__init__() # call parent class constructor\n",
        "        self.layer1 = nn.Linear(num_feature,2)  # define layer1\n",
        "        self.layer2 = nn.Linear(2,1)   # define layer2\n",
        "        self.sigmoid = nn.Sigmoid()    # define activation function\n",
        "\n",
        "        self.loss_function = nn.BCELoss() # define loss\n",
        "\n",
        "\n",
        "    def forward(self,x) :\n",
        "        y_pred = self.layer1(x)\n",
        "        y_pred = self.sigmoid(y_pred)\n",
        "        y_pred = self.layer2(y_pred)\n",
        "        y_pred = self.sigmoid(y_pred)\n",
        "        return y_pred\n",
        "\n",
        "model = Model(X.shape[1])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)  #model.parameter is an iterator to give all the trainable parameter\n",
        "\n",
        "for i in range(1000) :\n",
        "    y_hat = model(X)\n",
        "    loss = model.loss_function(y_hat,y.view(-1,1)) # view function work same as the calculate the loss\n",
        "    optimizer.zero_grad()  # gradient is zero\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "Pi5_xR2HrcF_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----Weight and bais Layer1-----\")\n",
        "print(model.layer1.weight)\n",
        "print(model.layer1.bias)\n",
        "print(\"----Weight and bais Layer2-----\")\n",
        "print(model.layer2.weight)\n",
        "print(model.layer2.bias)"
      ],
      "metadata": {
        "id": "bvYBSt4Dxk_n",
        "outputId": "5e856377-6c39-4d26-ada2-d088645c2bea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----Weight and bais Layer1-----\n",
            "Parameter containing:\n",
            "tensor([[ 0.1281,  0.1376,  0.1337,  0.0605, -0.1788,  0.0671, -0.0352,  0.0374,\n",
            "         -0.0349, -0.0321, -0.0048, -0.0761, -0.1150, -0.1315, -0.0944,  0.1294,\n",
            "          0.0831,  0.0467,  0.1393,  0.1156,  0.0414, -0.0368,  0.1283, -0.1048,\n",
            "         -0.0197,  0.1269, -0.0006,  0.0256, -0.0170,  0.0506],\n",
            "        [ 0.1451, -0.1454,  0.1500, -0.0258,  0.0342,  0.1516, -0.1669,  0.0075,\n",
            "          0.0630, -0.1511, -0.0636, -0.1259, -0.1595, -0.0735, -0.1038,  0.0446,\n",
            "          0.0367, -0.0946,  0.0818, -0.0407,  0.0954, -0.0049,  0.0677,  0.0415,\n",
            "          0.0742, -0.1383, -0.1736,  0.1353,  0.1600, -0.1077]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0618, -0.0766], requires_grad=True)\n",
            "----Weight and bais Layer2-----\n",
            "Parameter containing:\n",
            "tensor([[ 0.6186, -0.5790]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.6683], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model,input_size=X.shape)"
      ],
      "metadata": {
        "id": "zvH5Ti7v2Ryb",
        "outputId": "ab7de554-dc48-4079-d1f4-997745e95133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model                                    [569, 1]                  --\n",
              "├─Linear: 1-1                            [569, 2]                  62\n",
              "├─Sigmoid: 1-2                           [569, 2]                  --\n",
              "├─Linear: 1-3                            [569, 1]                  3\n",
              "├─Sigmoid: 1-4                           [569, 1]                  --\n",
              "==========================================================================================\n",
              "Total params: 65\n",
              "Trainable params: 65\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.04\n",
              "==========================================================================================\n",
              "Input size (MB): 0.07\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.08\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complex neural Net (With hidden layer)"
      ],
      "metadata": {
        "id": "OEuKgSfN9ppX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create more complex neural nertwork with moore hidden layer\n",
        "\n",
        "class Model1(nn.Module) :\n",
        "\n",
        "    def __init__(self,num_feature) :\n",
        "\n",
        "        super().__init__() # call parent class constructor\n",
        "        self.layer1 = nn.Linear(num_feature,25)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(25,12)\n",
        "        self.layer3 = nn.Linear(12,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x) :\n",
        "        y_pred = self.layer1(x)\n",
        "        y_pred = self.relu(y_pred)\n",
        "        y_pred = self.layer2(y_pred)\n",
        "        y_pred = self.relu(y_pred)\n",
        "        y_pred = self.layer3(y_pred)\n",
        "        y_pred = self.sigmoid(y_pred)\n",
        "        return y_pred\n",
        "\n",
        "model1 = Model1(X.shape[1])\n",
        "y_hat1 = model1(X)\n",
        "print(summary(model1,input_size=X.shape))\n",
        "\n",
        "\n",
        "# Here 1 problem this is samll neural net or less hidden layer if we have more hidden layer and complex neural net then this is tedius process to to define layer and activation function\n",
        "# Here we use sequential"
      ],
      "metadata": {
        "id": "yU_acms22nKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c94239-f59f-4ed7-b606-4286fb013985"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "Model1                                   [569, 1]                  --\n",
            "├─Linear: 1-1                            [569, 25]                 775\n",
            "├─ReLU: 1-2                              [569, 25]                 --\n",
            "├─Linear: 1-3                            [569, 12]                 312\n",
            "├─ReLU: 1-4                              [569, 12]                 --\n",
            "├─Linear: 1-5                            [569, 1]                  13\n",
            "├─Sigmoid: 1-6                           [569, 1]                  --\n",
            "==========================================================================================\n",
            "Total params: 1,100\n",
            "Trainable params: 1,100\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 0.63\n",
            "==========================================================================================\n",
            "Input size (MB): 0.07\n",
            "Forward/backward pass size (MB): 0.17\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.25\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define neural net using container\n",
        "\n"
      ],
      "metadata": {
        "id": "wZSWgWIG77Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model1(nn.Module) :\n",
        "    '''\n",
        "     Explenation of nn.Sequential  : The nn.Sequential module in PyTorch allows you to stack layers in a predefined order without explicitly writing each layer's forward pass.\n",
        "     This makes it easier to build simple feedforward networks.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,num_feature) :\n",
        "\n",
        "        super().__init__() # call parent class constructor\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(num_feature,25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25,12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.loss_function1 = nn.BCELoss()\n",
        "    def forward(self,x) :\n",
        "        y_pred = self.network(x)\n",
        "        return y_pred                       #Here, we don't need to manually define each layer. Once we create a Sequential module, it automatically defines the neural network structure.\n",
        "\n",
        "model1 = Model1(X.shape[1])\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(),lr=0.01)  #model.parameter is an iterator to give all the trainable parameter\n",
        "for i in range(1000) :\n",
        "    y_hat = model1(X)\n",
        "    loss1 = model1.loss_function1(y_hat,y.view(-1,1)) # view function work same as the calculate the loss\n",
        "    optimizer1.zero_grad()  # gradient is zero\n",
        "    loss1.backward()\n",
        "    optimizer1.step()\n",
        "    print(\"Epochs : \",i,\" \",'Loss : ',loss1.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baa9bxUu4-kS",
        "outputId": "72ff7f46-db88-47dd-ec11-227c9caf0649"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs :  0   Loss :  0.8486459255218506\n",
            "Epochs :  1   Loss :  27.099905014038086\n",
            "Epochs :  2   Loss :  25.342174530029297\n",
            "Epochs :  3   Loss :  15.061285018920898\n",
            "Epochs :  4   Loss :  1.6316688060760498\n",
            "Epochs :  5   Loss :  2.2016522884368896\n",
            "Epochs :  6   Loss :  3.1514077186584473\n",
            "Epochs :  7   Loss :  2.7868692874908447\n",
            "Epochs :  8   Loss :  2.0729081630706787\n",
            "Epochs :  9   Loss :  1.2583277225494385\n",
            "Epochs :  10   Loss :  0.7341236472129822\n",
            "Epochs :  11   Loss :  0.7197425365447998\n",
            "Epochs :  12   Loss :  1.0198312997817993\n",
            "Epochs :  13   Loss :  1.1203569173812866\n",
            "Epochs :  14   Loss :  0.953383207321167\n",
            "Epochs :  15   Loss :  0.7325643301010132\n",
            "Epochs :  16   Loss :  0.5918753147125244\n",
            "Epochs :  17   Loss :  0.5373269319534302\n",
            "Epochs :  18   Loss :  0.4826829731464386\n",
            "Epochs :  19   Loss :  0.4439253807067871\n",
            "Epochs :  20   Loss :  0.4640454649925232\n",
            "Epochs :  21   Loss :  0.4695962965488434\n",
            "Epochs :  22   Loss :  0.4350935220718384\n",
            "Epochs :  23   Loss :  0.4280313551425934\n",
            "Epochs :  24   Loss :  0.43975213170051575\n",
            "Epochs :  25   Loss :  0.4294225573539734\n",
            "Epochs :  26   Loss :  0.40265482664108276\n",
            "Epochs :  27   Loss :  0.3848735988140106\n",
            "Epochs :  28   Loss :  0.35932043194770813\n",
            "Epochs :  29   Loss :  0.3421495258808136\n",
            "Epochs :  30   Loss :  0.3355189561843872\n",
            "Epochs :  31   Loss :  0.31493282318115234\n",
            "Epochs :  32   Loss :  0.30868202447891235\n",
            "Epochs :  33   Loss :  0.29222139716148376\n",
            "Epochs :  34   Loss :  0.2828254699707031\n",
            "Epochs :  35   Loss :  0.27229660749435425\n",
            "Epochs :  36   Loss :  0.2620716989040375\n",
            "Epochs :  37   Loss :  0.2560535669326782\n",
            "Epochs :  38   Loss :  0.2456427663564682\n",
            "Epochs :  39   Loss :  0.24262940883636475\n",
            "Epochs :  40   Loss :  0.23408006131649017\n",
            "Epochs :  41   Loss :  0.23274193704128265\n",
            "Epochs :  42   Loss :  0.22605077922344208\n",
            "Epochs :  43   Loss :  0.22575600445270538\n",
            "Epochs :  44   Loss :  0.22085711359977722\n",
            "Epochs :  45   Loss :  0.22124913334846497\n",
            "Epochs :  46   Loss :  0.21770992875099182\n",
            "Epochs :  47   Loss :  0.21849462389945984\n",
            "Epochs :  48   Loss :  0.21584783494472504\n",
            "Epochs :  49   Loss :  0.21676522493362427\n",
            "Epochs :  50   Loss :  0.21475249528884888\n",
            "Epochs :  51   Loss :  0.21559639275074005\n",
            "Epochs :  52   Loss :  0.21398794651031494\n",
            "Epochs :  53   Loss :  0.21449406445026398\n",
            "Epochs :  54   Loss :  0.21319234371185303\n",
            "Epochs :  55   Loss :  0.21327075362205505\n",
            "Epochs :  56   Loss :  0.21222586929798126\n",
            "Epochs :  57   Loss :  0.21177417039871216\n",
            "Epochs :  58   Loss :  0.21093766391277313\n",
            "Epochs :  59   Loss :  0.21000972390174866\n",
            "Epochs :  60   Loss :  0.20933325588703156\n",
            "Epochs :  61   Loss :  0.20805925130844116\n",
            "Epochs :  62   Loss :  0.2074422687292099\n",
            "Epochs :  63   Loss :  0.2059997022151947\n",
            "Epochs :  64   Loss :  0.20532011985778809\n",
            "Epochs :  65   Loss :  0.2039305716753006\n",
            "Epochs :  66   Loss :  0.20308667421340942\n",
            "Epochs :  67   Loss :  0.20190462470054626\n",
            "Epochs :  68   Loss :  0.20086228847503662\n",
            "Epochs :  69   Loss :  0.19992628693580627\n",
            "Epochs :  70   Loss :  0.19876278936862946\n",
            "Epochs :  71   Loss :  0.19797272980213165\n",
            "Epochs :  72   Loss :  0.19685521721839905\n",
            "Epochs :  73   Loss :  0.19605755805969238\n",
            "Epochs :  74   Loss :  0.19513024389743805\n",
            "Epochs :  75   Loss :  0.194233700633049\n",
            "Epochs :  76   Loss :  0.193491593003273\n",
            "Epochs :  77   Loss :  0.19256949424743652\n",
            "Epochs :  78   Loss :  0.19185975193977356\n",
            "Epochs :  79   Loss :  0.19105090200901031\n",
            "Epochs :  80   Loss :  0.19025380909442902\n",
            "Epochs :  81   Loss :  0.18955902755260468\n",
            "Epochs :  82   Loss :  0.18873871862888336\n",
            "Epochs :  83   Loss :  0.18801410496234894\n",
            "Epochs :  84   Loss :  0.18727776408195496\n",
            "Epochs :  85   Loss :  0.18647240102291107\n",
            "Epochs :  86   Loss :  0.1857505738735199\n",
            "Epochs :  87   Loss :  0.18497934937477112\n",
            "Epochs :  88   Loss :  0.1841754913330078\n",
            "Epochs :  89   Loss :  0.1834287941455841\n",
            "Epochs :  90   Loss :  0.1826372742652893\n",
            "Epochs :  91   Loss :  0.18181969225406647\n",
            "Epochs :  92   Loss :  0.18104369938373566\n",
            "Epochs :  93   Loss :  0.18024450540542603\n",
            "Epochs :  94   Loss :  0.1794121414422989\n",
            "Epochs :  95   Loss :  0.17860552668571472\n",
            "Epochs :  96   Loss :  0.17780475318431854\n",
            "Epochs :  97   Loss :  0.1769731342792511\n",
            "Epochs :  98   Loss :  0.1761392503976822\n",
            "Epochs :  99   Loss :  0.17532534897327423\n",
            "Epochs :  100   Loss :  0.17451024055480957\n",
            "Epochs :  101   Loss :  0.1736789047718048\n",
            "Epochs :  102   Loss :  0.17284272611141205\n",
            "Epochs :  103   Loss :  0.17201755940914154\n",
            "Epochs :  104   Loss :  0.17120160162448883\n",
            "Epochs :  105   Loss :  0.17038452625274658\n",
            "Epochs :  106   Loss :  0.16956323385238647\n",
            "Epochs :  107   Loss :  0.1687379777431488\n",
            "Epochs :  108   Loss :  0.16791395843029022\n",
            "Epochs :  109   Loss :  0.16709256172180176\n",
            "Epochs :  110   Loss :  0.16627532243728638\n",
            "Epochs :  111   Loss :  0.16546139121055603\n",
            "Epochs :  112   Loss :  0.16465111076831818\n",
            "Epochs :  113   Loss :  0.16384421288967133\n",
            "Epochs :  114   Loss :  0.16304104030132294\n",
            "Epochs :  115   Loss :  0.1622420698404312\n",
            "Epochs :  116   Loss :  0.1614484339952469\n",
            "Epochs :  117   Loss :  0.16066253185272217\n",
            "Epochs :  118   Loss :  0.15989503264427185\n",
            "Epochs :  119   Loss :  0.15919293463230133\n",
            "Epochs :  120   Loss :  0.15881302952766418\n",
            "Epochs :  121   Loss :  0.16013719141483307\n",
            "Epochs :  122   Loss :  0.17159759998321533\n",
            "Epochs :  123   Loss :  0.22296205163002014\n",
            "Epochs :  124   Loss :  0.3689281642436981\n",
            "Epochs :  125   Loss :  0.2191094011068344\n",
            "Epochs :  126   Loss :  0.173627108335495\n",
            "Epochs :  127   Loss :  0.261248379945755\n",
            "Epochs :  128   Loss :  0.15488876402378082\n",
            "Epochs :  129   Loss :  0.24616765975952148\n",
            "Epochs :  130   Loss :  0.20672552287578583\n",
            "Epochs :  131   Loss :  0.19485321640968323\n",
            "Epochs :  132   Loss :  0.21649694442749023\n",
            "Epochs :  133   Loss :  0.1577579826116562\n",
            "Epochs :  134   Loss :  0.20564542710781097\n",
            "Epochs :  135   Loss :  0.15560781955718994\n",
            "Epochs :  136   Loss :  0.20325663685798645\n",
            "Epochs :  137   Loss :  0.1532103568315506\n",
            "Epochs :  138   Loss :  0.18806986510753632\n",
            "Epochs :  139   Loss :  0.15237322449684143\n",
            "Epochs :  140   Loss :  0.18324226140975952\n",
            "Epochs :  141   Loss :  0.15428070724010468\n",
            "Epochs :  142   Loss :  0.17431621253490448\n",
            "Epochs :  143   Loss :  0.15260373055934906\n",
            "Epochs :  144   Loss :  0.17010995745658875\n",
            "Epochs :  145   Loss :  0.15370532870292664\n",
            "Epochs :  146   Loss :  0.16487650573253632\n",
            "Epochs :  147   Loss :  0.1521053910255432\n",
            "Epochs :  148   Loss :  0.16153600811958313\n",
            "Epochs :  149   Loss :  0.15247510373592377\n",
            "Epochs :  150   Loss :  0.1581706702709198\n",
            "Epochs :  151   Loss :  0.15123139321804047\n",
            "Epochs :  152   Loss :  0.15544554591178894\n",
            "Epochs :  153   Loss :  0.1511979103088379\n",
            "Epochs :  154   Loss :  0.15309558808803558\n",
            "Epochs :  155   Loss :  0.15028968453407288\n",
            "Epochs :  156   Loss :  0.15082816779613495\n",
            "Epochs :  157   Loss :  0.14996998012065887\n",
            "Epochs :  158   Loss :  0.14910395443439484\n",
            "Epochs :  159   Loss :  0.14926952123641968\n",
            "Epochs :  160   Loss :  0.14730358123779297\n",
            "Epochs :  161   Loss :  0.14863499999046326\n",
            "Epochs :  162   Loss :  0.14605356752872467\n",
            "Epochs :  163   Loss :  0.14790669083595276\n",
            "Epochs :  164   Loss :  0.14485375583171844\n",
            "Epochs :  165   Loss :  0.14689147472381592\n",
            "Epochs :  166   Loss :  0.14405494928359985\n",
            "Epochs :  167   Loss :  0.14587193727493286\n",
            "Epochs :  168   Loss :  0.14349272847175598\n",
            "Epochs :  169   Loss :  0.14456596970558167\n",
            "Epochs :  170   Loss :  0.14305788278579712\n",
            "Epochs :  171   Loss :  0.14327259361743927\n",
            "Epochs :  172   Loss :  0.1427352875471115\n",
            "Epochs :  173   Loss :  0.1420661211013794\n",
            "Epochs :  174   Loss :  0.1422731578350067\n",
            "Epochs :  175   Loss :  0.14105060696601868\n",
            "Epochs :  176   Loss :  0.14155253767967224\n",
            "Epochs :  177   Loss :  0.14035096764564514\n",
            "Epochs :  178   Loss :  0.14063073694705963\n",
            "Epochs :  179   Loss :  0.1398799568414688\n",
            "Epochs :  180   Loss :  0.1395818144083023\n",
            "Epochs :  181   Loss :  0.13941146433353424\n",
            "Epochs :  182   Loss :  0.13865317404270172\n",
            "Epochs :  183   Loss :  0.13874341547489166\n",
            "Epochs :  184   Loss :  0.13798308372497559\n",
            "Epochs :  185   Loss :  0.1378750056028366\n",
            "Epochs :  186   Loss :  0.1374712437391281\n",
            "Epochs :  187   Loss :  0.13697591423988342\n",
            "Epochs :  188   Loss :  0.13687434792518616\n",
            "Epochs :  189   Loss :  0.13625957071781158\n",
            "Epochs :  190   Loss :  0.1360713243484497\n",
            "Epochs :  191   Loss :  0.13569582998752594\n",
            "Epochs :  192   Loss :  0.13523726165294647\n",
            "Epochs :  193   Loss :  0.13505426049232483\n",
            "Epochs :  194   Loss :  0.13456371426582336\n",
            "Epochs :  195   Loss :  0.13426519930362701\n",
            "Epochs :  196   Loss :  0.13397067785263062\n",
            "Epochs :  197   Loss :  0.1335117369890213\n",
            "Epochs :  198   Loss :  0.1332608163356781\n",
            "Epochs :  199   Loss :  0.13289019465446472\n",
            "Epochs :  200   Loss :  0.13249091804027557\n",
            "Epochs :  201   Loss :  0.13222616910934448\n",
            "Epochs :  202   Loss :  0.13183265924453735\n",
            "Epochs :  203   Loss :  0.13146978616714478\n",
            "Epochs :  204   Loss :  0.13118168711662292\n",
            "Epochs :  205   Loss :  0.13079169392585754\n",
            "Epochs :  206   Loss :  0.13044115900993347\n",
            "Epochs :  207   Loss :  0.13013750314712524\n",
            "Epochs :  208   Loss :  0.12975895404815674\n",
            "Epochs :  209   Loss :  0.1294070929288864\n",
            "Epochs :  210   Loss :  0.1290949136018753\n",
            "Epochs :  211   Loss :  0.1287296563386917\n",
            "Epochs :  212   Loss :  0.1283712536096573\n",
            "Epochs :  213   Loss :  0.12805169820785522\n",
            "Epochs :  214   Loss :  0.12770111858844757\n",
            "Epochs :  215   Loss :  0.12733738124370575\n",
            "Epochs :  216   Loss :  0.12700632214546204\n",
            "Epochs :  217   Loss :  0.12666922807693481\n",
            "Epochs :  218   Loss :  0.12630803883075714\n",
            "Epochs :  219   Loss :  0.12596160173416138\n",
            "Epochs :  220   Loss :  0.12562918663024902\n",
            "Epochs :  221   Loss :  0.125280499458313\n",
            "Epochs :  222   Loss :  0.12492456287145615\n",
            "Epochs :  223   Loss :  0.12458262592554092\n",
            "Epochs :  224   Loss :  0.12424483895301819\n",
            "Epochs :  225   Loss :  0.12389568239450455\n",
            "Epochs :  226   Loss :  0.12354303896427155\n",
            "Epochs :  227   Loss :  0.12319914251565933\n",
            "Epochs :  228   Loss :  0.12285919487476349\n",
            "Epochs :  229   Loss :  0.12251316756010056\n",
            "Epochs :  230   Loss :  0.12216256558895111\n",
            "Epochs :  231   Loss :  0.12181508541107178\n",
            "Epochs :  232   Loss :  0.12147260457277298\n",
            "Epochs :  233   Loss :  0.12113019078969955\n",
            "Epochs :  234   Loss :  0.12078394740819931\n",
            "Epochs :  235   Loss :  0.12043575942516327\n",
            "Epochs :  236   Loss :  0.12008898705244064\n",
            "Epochs :  237   Loss :  0.11974509060382843\n",
            "Epochs :  238   Loss :  0.11940265446901321\n",
            "Epochs :  239   Loss :  0.11905961483716965\n",
            "Epochs :  240   Loss :  0.11871535331010818\n",
            "Epochs :  241   Loss :  0.1183701604604721\n",
            "Epochs :  242   Loss :  0.11802531033754349\n",
            "Epochs :  243   Loss :  0.1176813468337059\n",
            "Epochs :  244   Loss :  0.11733854562044144\n",
            "Epochs :  245   Loss :  0.11699679493904114\n",
            "Epochs :  246   Loss :  0.11665600538253784\n",
            "Epochs :  247   Loss :  0.11631595343351364\n",
            "Epochs :  248   Loss :  0.11597690731287003\n",
            "Epochs :  249   Loss :  0.11563920229673386\n",
            "Epochs :  250   Loss :  0.11530357599258423\n",
            "Epochs :  251   Loss :  0.11497186124324799\n",
            "Epochs :  252   Loss :  0.11464755237102509\n",
            "Epochs :  253   Loss :  0.11434047669172287\n",
            "Epochs :  254   Loss :  0.11407352983951569\n",
            "Epochs :  255   Loss :  0.11391105502843857\n",
            "Epochs :  256   Loss :  0.11400748044252396\n",
            "Epochs :  257   Loss :  0.11489424109458923\n",
            "Epochs :  258   Loss :  0.11777640879154205\n",
            "Epochs :  259   Loss :  0.12788161635398865\n",
            "Epochs :  260   Loss :  0.1531839519739151\n",
            "Epochs :  261   Loss :  0.24382427334785461\n",
            "Epochs :  262   Loss :  0.32480835914611816\n",
            "Epochs :  263   Loss :  0.4571739137172699\n",
            "Epochs :  264   Loss :  0.17801302671432495\n",
            "Epochs :  265   Loss :  0.16152802109718323\n",
            "Epochs :  266   Loss :  0.3085194528102875\n",
            "Epochs :  267   Loss :  0.11480686068534851\n",
            "Epochs :  268   Loss :  0.2653959095478058\n",
            "Epochs :  269   Loss :  0.2564891576766968\n",
            "Epochs :  270   Loss :  0.16293177008628845\n",
            "Epochs :  271   Loss :  0.291258841753006\n",
            "Epochs :  272   Loss :  0.11837200075387955\n",
            "Epochs :  273   Loss :  0.2229221612215042\n",
            "Epochs :  274   Loss :  0.1178184300661087\n",
            "Epochs :  275   Loss :  0.20755042135715485\n",
            "Epochs :  276   Loss :  0.13944034278392792\n",
            "Epochs :  277   Loss :  0.18097873032093048\n",
            "Epochs :  278   Loss :  0.12825484573841095\n",
            "Epochs :  279   Loss :  0.16797393560409546\n",
            "Epochs :  280   Loss :  0.13489417731761932\n",
            "Epochs :  281   Loss :  0.15895259380340576\n",
            "Epochs :  282   Loss :  0.12402523308992386\n",
            "Epochs :  283   Loss :  0.15302376449108124\n",
            "Epochs :  284   Loss :  0.12492259591817856\n",
            "Epochs :  285   Loss :  0.14717355370521545\n",
            "Epochs :  286   Loss :  0.1180131807923317\n",
            "Epochs :  287   Loss :  0.1438818722963333\n",
            "Epochs :  288   Loss :  0.1177285835146904\n",
            "Epochs :  289   Loss :  0.13826122879981995\n",
            "Epochs :  290   Loss :  0.11459138989448547\n",
            "Epochs :  291   Loss :  0.13535435497760773\n",
            "Epochs :  292   Loss :  0.11413869261741638\n",
            "Epochs :  293   Loss :  0.13032226264476776\n",
            "Epochs :  294   Loss :  0.11318083107471466\n",
            "Epochs :  295   Loss :  0.12753035128116608\n",
            "Epochs :  296   Loss :  0.11268538981676102\n",
            "Epochs :  297   Loss :  0.12369117885828018\n",
            "Epochs :  298   Loss :  0.11243140697479248\n",
            "Epochs :  299   Loss :  0.12135841697454453\n",
            "Epochs :  300   Loss :  0.11186128854751587\n",
            "Epochs :  301   Loss :  0.11872480064630508\n",
            "Epochs :  302   Loss :  0.11159629374742508\n",
            "Epochs :  303   Loss :  0.11701098084449768\n",
            "Epochs :  304   Loss :  0.11097422242164612\n",
            "Epochs :  305   Loss :  0.11527641117572784\n",
            "Epochs :  306   Loss :  0.11056859791278839\n",
            "Epochs :  307   Loss :  0.11405482143163681\n",
            "Epochs :  308   Loss :  0.10996684432029724\n",
            "Epochs :  309   Loss :  0.11284381151199341\n",
            "Epochs :  310   Loss :  0.10950158536434174\n",
            "Epochs :  311   Loss :  0.1119360402226448\n",
            "Epochs :  312   Loss :  0.10898206382989883\n",
            "Epochs :  313   Loss :  0.1109997034072876\n",
            "Epochs :  314   Loss :  0.10854394733905792\n",
            "Epochs :  315   Loss :  0.11023376882076263\n",
            "Epochs :  316   Loss :  0.10814347863197327\n",
            "Epochs :  317   Loss :  0.10942605137825012\n",
            "Epochs :  318   Loss :  0.10778017342090607\n",
            "Epochs :  319   Loss :  0.10870209336280823\n",
            "Epochs :  320   Loss :  0.10748395323753357\n",
            "Epochs :  321   Loss :  0.10796904563903809\n",
            "Epochs :  322   Loss :  0.1071849912405014\n",
            "Epochs :  323   Loss :  0.1072823777794838\n",
            "Epochs :  324   Loss :  0.1069125086069107\n",
            "Epochs :  325   Loss :  0.10665197670459747\n",
            "Epochs :  326   Loss :  0.10660174489021301\n",
            "Epochs :  327   Loss :  0.1060827225446701\n",
            "Epochs :  328   Loss :  0.10624368488788605\n",
            "Epochs :  329   Loss :  0.10561169683933258\n",
            "Epochs :  330   Loss :  0.10583179444074631\n",
            "Epochs :  331   Loss :  0.10522212833166122\n",
            "Epochs :  332   Loss :  0.10535641759634018\n",
            "Epochs :  333   Loss :  0.10489724576473236\n",
            "Epochs :  334   Loss :  0.10486161708831787\n",
            "Epochs :  335   Loss :  0.10460421442985535\n",
            "Epochs :  336   Loss :  0.10438022762537003\n",
            "Epochs :  337   Loss :  0.10429190844297409\n",
            "Epochs :  338   Loss :  0.1039484441280365\n",
            "Epochs :  339   Loss :  0.10393194109201431\n",
            "Epochs :  340   Loss :  0.10358504205942154\n",
            "Epochs :  341   Loss :  0.10352982580661774\n",
            "Epochs :  342   Loss :  0.10327015072107315\n",
            "Epochs :  343   Loss :  0.10311267524957657\n",
            "Epochs :  344   Loss :  0.10296273231506348\n",
            "Epochs :  345   Loss :  0.10272078216075897\n",
            "Epochs :  346   Loss :  0.10262911021709442\n",
            "Epochs :  347   Loss :  0.10237650573253632\n",
            "Epochs :  348   Loss :  0.10226365178823471\n",
            "Epochs :  349   Loss :  0.10206687450408936\n",
            "Epochs :  350   Loss :  0.10189241915941238\n",
            "Epochs :  351   Loss :  0.10175763070583344\n",
            "Epochs :  352   Loss :  0.10154697299003601\n",
            "Epochs :  353   Loss :  0.10142543911933899\n",
            "Epochs :  354   Loss :  0.10123471915721893\n",
            "Epochs :  355   Loss :  0.10108083486557007\n",
            "Epochs :  356   Loss :  0.10093344002962112\n",
            "Epochs :  357   Loss :  0.1007504016160965\n",
            "Epochs :  358   Loss :  0.10061957687139511\n",
            "Epochs :  359   Loss :  0.10044556111097336\n",
            "Epochs :  360   Loss :  0.10029523819684982\n",
            "Epochs :  361   Loss :  0.10015076398849487\n",
            "Epochs :  362   Loss :  0.09998255968093872\n",
            "Epochs :  363   Loss :  0.09984730929136276\n",
            "Epochs :  364   Loss :  0.09968971461057663\n",
            "Epochs :  365   Loss :  0.09953958541154861\n",
            "Epochs :  366   Loss :  0.09940167516469955\n",
            "Epochs :  367   Loss :  0.09924501180648804\n",
            "Epochs :  368   Loss :  0.0991070568561554\n",
            "Epochs :  369   Loss :  0.09896368533372879\n",
            "Epochs :  370   Loss :  0.09881586581468582\n",
            "Epochs :  371   Loss :  0.09868174046278\n",
            "Epochs :  372   Loss :  0.09853768348693848\n",
            "Epochs :  373   Loss :  0.09839858114719391\n",
            "Epochs :  374   Loss :  0.098265141248703\n",
            "Epochs :  375   Loss :  0.09812422096729279\n",
            "Epochs :  376   Loss :  0.09799110889434814\n",
            "Epochs :  377   Loss :  0.09785845130681992\n",
            "Epochs :  378   Loss :  0.0977223739027977\n",
            "Epochs :  379   Loss :  0.0975930318236351\n",
            "Epochs :  380   Loss :  0.09746240079402924\n",
            "Epochs :  381   Loss :  0.09733102470636368\n",
            "Epochs :  382   Loss :  0.09720463305711746\n",
            "Epochs :  383   Loss :  0.09707687795162201\n",
            "Epochs :  384   Loss :  0.09694968909025192\n",
            "Epochs :  385   Loss :  0.09682627767324448\n",
            "Epochs :  386   Loss :  0.09670179337263107\n",
            "Epochs :  387   Loss :  0.09657827019691467\n",
            "Epochs :  388   Loss :  0.09645766764879227\n",
            "Epochs :  389   Loss :  0.09633663296699524\n",
            "Epochs :  390   Loss :  0.0962163582444191\n",
            "Epochs :  391   Loss :  0.09609875082969666\n",
            "Epochs :  392   Loss :  0.09598104655742645\n",
            "Epochs :  393   Loss :  0.09586405009031296\n",
            "Epochs :  394   Loss :  0.09574923664331436\n",
            "Epochs :  395   Loss :  0.095634825527668\n",
            "Epochs :  396   Loss :  0.09552112221717834\n",
            "Epochs :  397   Loss :  0.095409095287323\n",
            "Epochs :  398   Loss :  0.0952979326248169\n",
            "Epochs :  399   Loss :  0.09518728405237198\n",
            "Epochs :  400   Loss :  0.09507810324430466\n",
            "Epochs :  401   Loss :  0.09497012943029404\n",
            "Epochs :  402   Loss :  0.09486253559589386\n",
            "Epochs :  403   Loss :  0.09475614130496979\n",
            "Epochs :  404   Loss :  0.09465105831623077\n",
            "Epochs :  405   Loss :  0.09454657137393951\n",
            "Epochs :  406   Loss :  0.09444290399551392\n",
            "Epochs :  407   Loss :  0.09434060007333755\n",
            "Epochs :  408   Loss :  0.09423913806676865\n",
            "Epochs :  409   Loss :  0.0941382572054863\n",
            "Epochs :  410   Loss :  0.09403860569000244\n",
            "Epochs :  411   Loss :  0.09393997490406036\n",
            "Epochs :  412   Loss :  0.09384207427501678\n",
            "Epochs :  413   Loss :  0.09374495595693588\n",
            "Epochs :  414   Loss :  0.09364891052246094\n",
            "Epochs :  415   Loss :  0.0935538038611412\n",
            "Epochs :  416   Loss :  0.09345942735671997\n",
            "Epochs :  417   Loss :  0.093365877866745\n",
            "Epochs :  418   Loss :  0.09327328205108643\n",
            "Epochs :  419   Loss :  0.0931815579533577\n",
            "Epochs :  420   Loss :  0.09309060126543045\n",
            "Epochs :  421   Loss :  0.09300056844949722\n",
            "Epochs :  422   Loss :  0.09291133284568787\n",
            "Epochs :  423   Loss :  0.09282288700342178\n",
            "Epochs :  424   Loss :  0.09273522347211838\n",
            "Epochs :  425   Loss :  0.09264840185642242\n",
            "Epochs :  426   Loss :  0.09256241470575333\n",
            "Epochs :  427   Loss :  0.09247709810733795\n",
            "Epochs :  428   Loss :  0.09239263087511063\n",
            "Epochs :  429   Loss :  0.0923088937997818\n",
            "Epochs :  430   Loss :  0.09222592413425446\n",
            "Epochs :  431   Loss :  0.09214373677968979\n",
            "Epochs :  432   Loss :  0.09206221997737885\n",
            "Epochs :  433   Loss :  0.0919814482331276\n",
            "Epochs :  434   Loss :  0.09190141409635544\n",
            "Epochs :  435   Loss :  0.09182210266590118\n",
            "Epochs :  436   Loss :  0.09174345433712006\n",
            "Epochs :  437   Loss :  0.09166552126407623\n",
            "Epochs :  438   Loss :  0.09158828854560852\n",
            "Epochs :  439   Loss :  0.09151174128055573\n",
            "Epochs :  440   Loss :  0.0914357453584671\n",
            "Epochs :  441   Loss :  0.09136053919792175\n",
            "Epochs :  442   Loss :  0.09128593653440475\n",
            "Epochs :  443   Loss :  0.09121200442314148\n",
            "Epochs :  444   Loss :  0.09113873541355133\n",
            "Epochs :  445   Loss :  0.09106602519750595\n",
            "Epochs :  446   Loss :  0.09099388122558594\n",
            "Epochs :  447   Loss :  0.09092246741056442\n",
            "Epochs :  448   Loss :  0.09085158258676529\n",
            "Epochs :  449   Loss :  0.0907813310623169\n",
            "Epochs :  450   Loss :  0.09071160107851028\n",
            "Epochs :  451   Loss :  0.09064245969057083\n",
            "Epochs :  452   Loss :  0.09057396650314331\n",
            "Epochs :  453   Loss :  0.09050597995519638\n",
            "Epochs :  454   Loss :  0.09043849259614944\n",
            "Epochs :  455   Loss :  0.09037155658006668\n",
            "Epochs :  456   Loss :  0.09030521661043167\n",
            "Epochs :  457   Loss :  0.09023937582969666\n",
            "Epochs :  458   Loss :  0.09017399698495865\n",
            "Epochs :  459   Loss :  0.09010918438434601\n",
            "Epochs :  460   Loss :  0.09004484117031097\n",
            "Epochs :  461   Loss :  0.08998101949691772\n",
            "Epochs :  462   Loss :  0.08991771191358566\n",
            "Epochs :  463   Loss :  0.08985486626625061\n",
            "Epochs :  464   Loss :  0.08979249000549316\n",
            "Epochs :  465   Loss :  0.08973058313131332\n",
            "Epochs :  466   Loss :  0.08966902643442154\n",
            "Epochs :  467   Loss :  0.0896080955862999\n",
            "Epochs :  468   Loss :  0.08954751491546631\n",
            "Epochs :  469   Loss :  0.08948734402656555\n",
            "Epochs :  470   Loss :  0.0894276574254036\n",
            "Epochs :  471   Loss :  0.08936836570501328\n",
            "Epochs :  472   Loss :  0.08930949866771698\n",
            "Epochs :  473   Loss :  0.08925101906061172\n",
            "Epochs :  474   Loss :  0.08919297903776169\n",
            "Epochs :  475   Loss :  0.08913528919219971\n",
            "Epochs :  476   Loss :  0.08907802402973175\n",
            "Epochs :  477   Loss :  0.08902111649513245\n",
            "Epochs :  478   Loss :  0.08896459639072418\n",
            "Epochs :  479   Loss :  0.08890844881534576\n",
            "Epochs :  480   Loss :  0.08885269612073898\n",
            "Epochs :  481   Loss :  0.08879727125167847\n",
            "Epochs :  482   Loss :  0.08874223381280899\n",
            "Epochs :  483   Loss :  0.08868764340877533\n",
            "Epochs :  484   Loss :  0.0886334553360939\n",
            "Epochs :  485   Loss :  0.08857984095811844\n",
            "Epochs :  486   Loss :  0.08852684497833252\n",
            "Epochs :  487   Loss :  0.08847488462924957\n",
            "Epochs :  488   Loss :  0.08842463791370392\n",
            "Epochs :  489   Loss :  0.08837725222110748\n",
            "Epochs :  490   Loss :  0.08833547681570053\n",
            "Epochs :  491   Loss :  0.08830475807189941\n",
            "Epochs :  492   Loss :  0.08829741179943085\n",
            "Epochs :  493   Loss :  0.08833777904510498\n",
            "Epochs :  494   Loss :  0.08848901093006134\n",
            "Epochs :  495   Loss :  0.08886846154928207\n",
            "Epochs :  496   Loss :  0.08983781188726425\n",
            "Epochs :  497   Loss :  0.0919940173625946\n",
            "Epochs :  498   Loss :  0.09780294448137283\n",
            "Epochs :  499   Loss :  0.11054936051368713\n",
            "Epochs :  500   Loss :  0.15272779762744904\n",
            "Epochs :  501   Loss :  0.24291211366653442\n",
            "Epochs :  502   Loss :  0.6539444923400879\n",
            "Epochs :  503   Loss :  1.5731645822525024\n",
            "Epochs :  504   Loss :  1.4648653268814087\n",
            "Epochs :  505   Loss :  0.09058523923158646\n",
            "Epochs :  506   Loss :  1.2381796836853027\n",
            "Epochs :  507   Loss :  0.25801390409469604\n",
            "Epochs :  508   Loss :  1.0422132015228271\n",
            "Epochs :  509   Loss :  0.10106009989976883\n",
            "Epochs :  510   Loss :  1.0091019868850708\n",
            "Epochs :  511   Loss :  0.11780992150306702\n",
            "Epochs :  512   Loss :  0.5213544368743896\n",
            "Epochs :  513   Loss :  0.46320614218711853\n",
            "Epochs :  514   Loss :  0.11274133622646332\n",
            "Epochs :  515   Loss :  0.4865090250968933\n",
            "Epochs :  516   Loss :  0.2328358143568039\n",
            "Epochs :  517   Loss :  0.1659652143716812\n",
            "Epochs :  518   Loss :  0.3643491864204407\n",
            "Epochs :  519   Loss :  0.23568181693553925\n",
            "Epochs :  520   Loss :  0.12358354777097702\n",
            "Epochs :  521   Loss :  0.2779293358325958\n",
            "Epochs :  522   Loss :  0.21306970715522766\n",
            "Epochs :  523   Loss :  0.12156446278095245\n",
            "Epochs :  524   Loss :  0.21045103669166565\n",
            "Epochs :  525   Loss :  0.21864864230155945\n",
            "Epochs :  526   Loss :  0.1316557675600052\n",
            "Epochs :  527   Loss :  0.14668798446655273\n",
            "Epochs :  528   Loss :  0.19989894330501556\n",
            "Epochs :  529   Loss :  0.14483797550201416\n",
            "Epochs :  530   Loss :  0.12406307458877563\n",
            "Epochs :  531   Loss :  0.16573184728622437\n",
            "Epochs :  532   Loss :  0.16059203445911407\n",
            "Epochs :  533   Loss :  0.12205198407173157\n",
            "Epochs :  534   Loss :  0.1318819671869278\n",
            "Epochs :  535   Loss :  0.15460485219955444\n",
            "Epochs :  536   Loss :  0.13124100863933563\n",
            "Epochs :  537   Loss :  0.11828947067260742\n",
            "Epochs :  538   Loss :  0.13654762506484985\n",
            "Epochs :  539   Loss :  0.1377728134393692\n",
            "Epochs :  540   Loss :  0.11935354024171829\n",
            "Epochs :  541   Loss :  0.1199127733707428\n",
            "Epochs :  542   Loss :  0.13206210732460022\n",
            "Epochs :  543   Loss :  0.12432891130447388\n",
            "Epochs :  544   Loss :  0.11477887630462646\n",
            "Epochs :  545   Loss :  0.12159299850463867\n",
            "Epochs :  546   Loss :  0.12506501376628876\n",
            "Epochs :  547   Loss :  0.11672646552324295\n",
            "Epochs :  548   Loss :  0.11413481831550598\n",
            "Epochs :  549   Loss :  0.12004322558641434\n",
            "Epochs :  550   Loss :  0.11846371740102768\n",
            "Epochs :  551   Loss :  0.11262470483779907\n",
            "Epochs :  552   Loss :  0.11435278505086899\n",
            "Epochs :  553   Loss :  0.11710014194250107\n",
            "Epochs :  554   Loss :  0.11363069713115692\n",
            "Epochs :  555   Loss :  0.1111331507563591\n",
            "Epochs :  556   Loss :  0.11359479278326035\n",
            "Epochs :  557   Loss :  0.11363961547613144\n",
            "Epochs :  558   Loss :  0.11051085591316223\n",
            "Epochs :  559   Loss :  0.11055639386177063\n",
            "Epochs :  560   Loss :  0.11206256598234177\n",
            "Epochs :  561   Loss :  0.11058243364095688\n",
            "Epochs :  562   Loss :  0.10892915725708008\n",
            "Epochs :  563   Loss :  0.10984618961811066\n",
            "Epochs :  564   Loss :  0.10998710989952087\n",
            "Epochs :  565   Loss :  0.10833735764026642\n",
            "Epochs :  566   Loss :  0.10807344317436218\n",
            "Epochs :  567   Loss :  0.10872653126716614\n",
            "Epochs :  568   Loss :  0.10792840272188187\n",
            "Epochs :  569   Loss :  0.1069650873541832\n",
            "Epochs :  570   Loss :  0.10728868842124939\n",
            "Epochs :  571   Loss :  0.10722405463457108\n",
            "Epochs :  572   Loss :  0.10628603398799896\n",
            "Epochs :  573   Loss :  0.10607762634754181\n",
            "Epochs :  574   Loss :  0.10626591742038727\n",
            "Epochs :  575   Loss :  0.10569743067026138\n",
            "Epochs :  576   Loss :  0.1051609069108963\n",
            "Epochs :  577   Loss :  0.10524939000606537\n",
            "Epochs :  578   Loss :  0.1050204485654831\n",
            "Epochs :  579   Loss :  0.10444778203964233\n",
            "Epochs :  580   Loss :  0.10431884974241257\n",
            "Epochs :  581   Loss :  0.10425534099340439\n",
            "Epochs :  582   Loss :  0.10380788892507553\n",
            "Epochs :  583   Loss :  0.10351572930812836\n",
            "Epochs :  584   Loss :  0.10346855223178864\n",
            "Epochs :  585   Loss :  0.10316973179578781\n",
            "Epochs :  586   Loss :  0.10282060503959656\n",
            "Epochs :  587   Loss :  0.10271377116441727\n",
            "Epochs :  588   Loss :  0.10251966863870621\n",
            "Epochs :  589   Loss :  0.1021866500377655\n",
            "Epochs :  590   Loss :  0.10201508551836014\n",
            "Epochs :  591   Loss :  0.10187032073736191\n",
            "Epochs :  592   Loss :  0.10158456861972809\n",
            "Epochs :  593   Loss :  0.10137154161930084\n",
            "Epochs :  594   Loss :  0.10123571008443832\n",
            "Epochs :  595   Loss :  0.10099918395280838\n",
            "Epochs :  596   Loss :  0.10076993703842163\n",
            "Epochs :  597   Loss :  0.10062546283006668\n",
            "Epochs :  598   Loss :  0.10042575746774673\n",
            "Epochs :  599   Loss :  0.10019975155591965\n",
            "Epochs :  600   Loss :  0.10004282742738724\n",
            "Epochs :  601   Loss :  0.09986653178930283\n",
            "Epochs :  602   Loss :  0.0996536836028099\n",
            "Epochs :  603   Loss :  0.09948901832103729\n",
            "Epochs :  604   Loss :  0.09932606667280197\n",
            "Epochs :  605   Loss :  0.09912867844104767\n",
            "Epochs :  606   Loss :  0.09896073490381241\n",
            "Epochs :  607   Loss :  0.09880486875772476\n",
            "Epochs :  608   Loss :  0.09862159192562103\n",
            "Epochs :  609   Loss :  0.09845494478940964\n",
            "Epochs :  610   Loss :  0.09830383956432343\n",
            "Epochs :  611   Loss :  0.09813273698091507\n",
            "Epochs :  612   Loss :  0.09797032177448273\n",
            "Epochs :  613   Loss :  0.09782280772924423\n",
            "Epochs :  614   Loss :  0.09766165167093277\n",
            "Epochs :  615   Loss :  0.09750469774007797\n",
            "Epochs :  616   Loss :  0.097360759973526\n",
            "Epochs :  617   Loss :  0.09720786660909653\n",
            "Epochs :  618   Loss :  0.09705691784620285\n",
            "Epochs :  619   Loss :  0.0969168022274971\n",
            "Epochs :  620   Loss :  0.0967709943652153\n",
            "Epochs :  621   Loss :  0.09662625193595886\n",
            "Epochs :  622   Loss :  0.09649014472961426\n",
            "Epochs :  623   Loss :  0.09635055810213089\n",
            "Epochs :  624   Loss :  0.09621177613735199\n",
            "Epochs :  625   Loss :  0.09607991576194763\n",
            "Epochs :  626   Loss :  0.09594594687223434\n",
            "Epochs :  627   Loss :  0.09581304341554642\n",
            "Epochs :  628   Loss :  0.09568560868501663\n",
            "Epochs :  629   Loss :  0.09555697441101074\n",
            "Epochs :  630   Loss :  0.09542958438396454\n",
            "Epochs :  631   Loss :  0.09530653059482574\n",
            "Epochs :  632   Loss :  0.0951828882098198\n",
            "Epochs :  633   Loss :  0.09506072103977203\n",
            "Epochs :  634   Loss :  0.09494201838970184\n",
            "Epochs :  635   Loss :  0.09482330083847046\n",
            "Epochs :  636   Loss :  0.09470638632774353\n",
            "Epochs :  637   Loss :  0.09459233283996582\n",
            "Epochs :  638   Loss :  0.09447821229696274\n",
            "Epochs :  639   Loss :  0.09436605125665665\n",
            "Epochs :  640   Loss :  0.09425611793994904\n",
            "Epochs :  641   Loss :  0.09414646774530411\n",
            "Epochs :  642   Loss :  0.09403877705335617\n",
            "Epochs :  643   Loss :  0.0939328745007515\n",
            "Epochs :  644   Loss :  0.09382753074169159\n",
            "Epochs :  645   Loss :  0.0937240943312645\n",
            "Epochs :  646   Loss :  0.09362207353115082\n",
            "Epochs :  647   Loss :  0.09352076798677444\n",
            "Epochs :  648   Loss :  0.09342136234045029\n",
            "Epochs :  649   Loss :  0.09332307428121567\n",
            "Epochs :  650   Loss :  0.09322578459978104\n",
            "Epochs :  651   Loss :  0.09313011914491653\n",
            "Epochs :  652   Loss :  0.0930355042219162\n",
            "Epochs :  653   Loss :  0.09294202923774719\n",
            "Epochs :  654   Loss :  0.09284993261098862\n",
            "Epochs :  655   Loss :  0.09275894612073898\n",
            "Epochs :  656   Loss :  0.0926690548658371\n",
            "Epochs :  657   Loss :  0.0925804153084755\n",
            "Epochs :  658   Loss :  0.09249278903007507\n",
            "Epochs :  659   Loss :  0.0924062430858612\n",
            "Epochs :  660   Loss :  0.09232085943222046\n",
            "Epochs :  661   Loss :  0.09223650395870209\n",
            "Epochs :  662   Loss :  0.09215323626995087\n",
            "Epochs :  663   Loss :  0.09207096695899963\n",
            "Epochs :  664   Loss :  0.09198971837759018\n",
            "Epochs :  665   Loss :  0.0919095054268837\n",
            "Epochs :  666   Loss :  0.09183023869991302\n",
            "Epochs :  667   Loss :  0.09175191819667816\n",
            "Epochs :  668   Loss :  0.09167462587356567\n",
            "Epochs :  669   Loss :  0.09159820526838303\n",
            "Epochs :  670   Loss :  0.09152272343635559\n",
            "Epochs :  671   Loss :  0.09144817292690277\n",
            "Epochs :  672   Loss :  0.09137449413537979\n",
            "Epochs :  673   Loss :  0.09130167216062546\n",
            "Epochs :  674   Loss :  0.09122978150844574\n",
            "Epochs :  675   Loss :  0.09115868806838989\n",
            "Epochs :  676   Loss :  0.09108845144510269\n",
            "Epochs :  677   Loss :  0.09101904928684235\n",
            "Epochs :  678   Loss :  0.09095044434070587\n",
            "Epochs :  679   Loss :  0.09088262915611267\n",
            "Epochs :  680   Loss :  0.09081555157899857\n",
            "Epochs :  681   Loss :  0.09074931591749191\n",
            "Epochs :  682   Loss :  0.09068380296230316\n",
            "Epochs :  683   Loss :  0.0906190350651741\n",
            "Epochs :  684   Loss :  0.09055497497320175\n",
            "Epochs :  685   Loss :  0.0904916301369667\n",
            "Epochs :  686   Loss :  0.09042903780937195\n",
            "Epochs :  687   Loss :  0.09036710858345032\n",
            "Epochs :  688   Loss :  0.0903058871626854\n",
            "Epochs :  689   Loss :  0.09024529159069061\n",
            "Epochs :  690   Loss :  0.09018541872501373\n",
            "Epochs :  691   Loss :  0.0901261493563652\n",
            "Epochs :  692   Loss :  0.09006748348474503\n",
            "Epochs :  693   Loss :  0.0900094211101532\n",
            "Epochs :  694   Loss :  0.0899520143866539\n",
            "Epochs :  695   Loss :  0.08989528566598892\n",
            "Epochs :  696   Loss :  0.08983907103538513\n",
            "Epochs :  697   Loss :  0.08978342264890671\n",
            "Epochs :  698   Loss :  0.08972838521003723\n",
            "Epochs :  699   Loss :  0.08967391401529312\n",
            "Epochs :  700   Loss :  0.0896199643611908\n",
            "Epochs :  701   Loss :  0.08956659585237503\n",
            "Epochs :  702   Loss :  0.08951371163129807\n",
            "Epochs :  703   Loss :  0.08946133404970169\n",
            "Epochs :  704   Loss :  0.08940950781106949\n",
            "Epochs :  705   Loss :  0.08935822546482086\n",
            "Epochs :  706   Loss :  0.08930738270282745\n",
            "Epochs :  707   Loss :  0.0892571434378624\n",
            "Epochs :  708   Loss :  0.08920735120773315\n",
            "Epochs :  709   Loss :  0.0891580581665039\n",
            "Epochs :  710   Loss :  0.08910919725894928\n",
            "Epochs :  711   Loss :  0.08906085789203644\n",
            "Epochs :  712   Loss :  0.08901292085647583\n",
            "Epochs :  713   Loss :  0.08896545320749283\n",
            "Epochs :  714   Loss :  0.08891835063695908\n",
            "Epochs :  715   Loss :  0.0888717770576477\n",
            "Epochs :  716   Loss :  0.08882558345794678\n",
            "Epochs :  717   Loss :  0.0887797474861145\n",
            "Epochs :  718   Loss :  0.08873441070318222\n",
            "Epochs :  719   Loss :  0.08868935704231262\n",
            "Epochs :  720   Loss :  0.08864480257034302\n",
            "Epochs :  721   Loss :  0.08860059082508087\n",
            "Epochs :  722   Loss :  0.08855670690536499\n",
            "Epochs :  723   Loss :  0.08851324766874313\n",
            "Epochs :  724   Loss :  0.08847015351057053\n",
            "Epochs :  725   Loss :  0.08842746168375015\n",
            "Epochs :  726   Loss :  0.08838503062725067\n",
            "Epochs :  727   Loss :  0.08834299445152283\n",
            "Epochs :  728   Loss :  0.08830128610134125\n",
            "Epochs :  729   Loss :  0.08825994282960892\n",
            "Epochs :  730   Loss :  0.08821884542703629\n",
            "Epochs :  731   Loss :  0.0881781056523323\n",
            "Epochs :  732   Loss :  0.08813773095607758\n",
            "Epochs :  733   Loss :  0.08809765428304672\n",
            "Epochs :  734   Loss :  0.08805790543556213\n",
            "Epochs :  735   Loss :  0.08801840245723724\n",
            "Epochs :  736   Loss :  0.08797920495271683\n",
            "Epochs :  737   Loss :  0.0879402756690979\n",
            "Epochs :  738   Loss :  0.08790174126625061\n",
            "Epochs :  739   Loss :  0.08786346018314362\n",
            "Epochs :  740   Loss :  0.08782535046339035\n",
            "Epochs :  741   Loss :  0.08778754621744156\n",
            "Epochs :  742   Loss :  0.08775009214878082\n",
            "Epochs :  743   Loss :  0.087712861597538\n",
            "Epochs :  744   Loss :  0.08767585456371307\n",
            "Epochs :  745   Loss :  0.08763913810253143\n",
            "Epochs :  746   Loss :  0.0876026377081871\n",
            "Epochs :  747   Loss :  0.08756645768880844\n",
            "Epochs :  748   Loss :  0.08753044903278351\n",
            "Epochs :  749   Loss :  0.08749475330114365\n",
            "Epochs :  750   Loss :  0.08745922893285751\n",
            "Epochs :  751   Loss :  0.08742392808198929\n",
            "Epochs :  752   Loss :  0.08738887310028076\n",
            "Epochs :  753   Loss :  0.08735406398773193\n",
            "Epochs :  754   Loss :  0.08731947094202042\n",
            "Epochs :  755   Loss :  0.08728515356779099\n",
            "Epochs :  756   Loss :  0.08725102245807648\n",
            "Epochs :  757   Loss :  0.08721715956926346\n",
            "Epochs :  758   Loss :  0.08718347549438477\n",
            "Epochs :  759   Loss :  0.08714998513460159\n",
            "Epochs :  760   Loss :  0.08711667358875275\n",
            "Epochs :  761   Loss :  0.0870836153626442\n",
            "Epochs :  762   Loss :  0.08705070614814758\n",
            "Epochs :  763   Loss :  0.08701800554990768\n",
            "Epochs :  764   Loss :  0.0869855210185051\n",
            "Epochs :  765   Loss :  0.08695311099290848\n",
            "Epochs :  766   Loss :  0.08692102134227753\n",
            "Epochs :  767   Loss :  0.08688903599977493\n",
            "Epochs :  768   Loss :  0.08685725182294846\n",
            "Epochs :  769   Loss :  0.08682562410831451\n",
            "Epochs :  770   Loss :  0.08679415285587311\n",
            "Epochs :  771   Loss :  0.08676287531852722\n",
            "Epochs :  772   Loss :  0.08673176914453506\n",
            "Epochs :  773   Loss :  0.08670076727867126\n",
            "Epochs :  774   Loss :  0.08667002618312836\n",
            "Epochs :  775   Loss :  0.08663936704397202\n",
            "Epochs :  776   Loss :  0.0866088792681694\n",
            "Epochs :  777   Loss :  0.08657854795455933\n",
            "Epochs :  778   Loss :  0.0865483507514\n",
            "Epochs :  779   Loss :  0.08651825785636902\n",
            "Epochs :  780   Loss :  0.08648838102817535\n",
            "Epochs :  781   Loss :  0.08645862340927124\n",
            "Epochs :  782   Loss :  0.08642908185720444\n",
            "Epochs :  783   Loss :  0.08639957755804062\n",
            "Epochs :  784   Loss :  0.08637019991874695\n",
            "Epochs :  785   Loss :  0.0863410010933876\n",
            "Epochs :  786   Loss :  0.08631192147731781\n",
            "Epochs :  787   Loss :  0.08628295361995697\n",
            "Epochs :  788   Loss :  0.08625411987304688\n",
            "Epochs :  789   Loss :  0.08622542768716812\n",
            "Epochs :  790   Loss :  0.08619686216115952\n",
            "Epochs :  791   Loss :  0.08616837114095688\n",
            "Epochs :  792   Loss :  0.08614004403352737\n",
            "Epochs :  793   Loss :  0.08611180633306503\n",
            "Epochs :  794   Loss :  0.08608368784189224\n",
            "Epochs :  795   Loss :  0.08605572581291199\n",
            "Epochs :  796   Loss :  0.08602776378393173\n",
            "Epochs :  797   Loss :  0.08600001782178879\n",
            "Epochs :  798   Loss :  0.08597234636545181\n",
            "Epochs :  799   Loss :  0.08594472706317902\n",
            "Epochs :  800   Loss :  0.08591730147600174\n",
            "Epochs :  801   Loss :  0.08588989824056625\n",
            "Epochs :  802   Loss :  0.0858626440167427\n",
            "Epochs :  803   Loss :  0.08583549410104752\n",
            "Epochs :  804   Loss :  0.08580843359231949\n",
            "Epochs :  805   Loss :  0.08578141033649445\n",
            "Epochs :  806   Loss :  0.08575452119112015\n",
            "Epochs :  807   Loss :  0.085727758705616\n",
            "Epochs :  808   Loss :  0.08570101112127304\n",
            "Epochs :  809   Loss :  0.0856742113828659\n",
            "Epochs :  810   Loss :  0.08564763516187668\n",
            "Epochs :  811   Loss :  0.08562107384204865\n",
            "Epochs :  812   Loss :  0.08559466898441315\n",
            "Epochs :  813   Loss :  0.08556824177503586\n",
            "Epochs :  814   Loss :  0.08554200828075409\n",
            "Epochs :  815   Loss :  0.0855158269405365\n",
            "Epochs :  816   Loss :  0.08548971265554428\n",
            "Epochs :  817   Loss :  0.08546372503042221\n",
            "Epochs :  818   Loss :  0.08543787151575089\n",
            "Epochs :  819   Loss :  0.08541204780340195\n",
            "Epochs :  820   Loss :  0.08538627624511719\n",
            "Epochs :  821   Loss :  0.08536052703857422\n",
            "Epochs :  822   Loss :  0.08533491939306259\n",
            "Epochs :  823   Loss :  0.08530931174755096\n",
            "Epochs :  824   Loss :  0.08528381586074829\n",
            "Epochs :  825   Loss :  0.08525832742452621\n",
            "Epochs :  826   Loss :  0.0852329283952713\n",
            "Epochs :  827   Loss :  0.08520759642124176\n",
            "Epochs :  828   Loss :  0.08518242090940475\n",
            "Epochs :  829   Loss :  0.08515728265047073\n",
            "Epochs :  830   Loss :  0.08513220399618149\n",
            "Epochs :  831   Loss :  0.08510713279247284\n",
            "Epochs :  832   Loss :  0.08508210629224777\n",
            "Epochs :  833   Loss :  0.08505719900131226\n",
            "Epochs :  834   Loss :  0.08503226935863495\n",
            "Epochs :  835   Loss :  0.08500754833221436\n",
            "Epochs :  836   Loss :  0.08498279750347137\n",
            "Epochs :  837   Loss :  0.08495809137821198\n",
            "Epochs :  838   Loss :  0.08493344485759735\n",
            "Epochs :  839   Loss :  0.0849088579416275\n",
            "Epochs :  840   Loss :  0.08488428592681885\n",
            "Epochs :  841   Loss :  0.08485984057188034\n",
            "Epochs :  842   Loss :  0.08483535051345825\n",
            "Epochs :  843   Loss :  0.08481105417013168\n",
            "Epochs :  844   Loss :  0.0847867801785469\n",
            "Epochs :  845   Loss :  0.08476253598928452\n",
            "Epochs :  846   Loss :  0.08473830670118332\n",
            "Epochs :  847   Loss :  0.08471417427062988\n",
            "Epochs :  848   Loss :  0.08469007909297943\n",
            "Epochs :  849   Loss :  0.08466599881649017\n",
            "Epochs :  850   Loss :  0.08464197814464569\n",
            "Epochs :  851   Loss :  0.08461792021989822\n",
            "Epochs :  852   Loss :  0.08459403365850449\n",
            "Epochs :  853   Loss :  0.08457013219594955\n",
            "Epochs :  854   Loss :  0.08454623818397522\n",
            "Epochs :  855   Loss :  0.08452247083187103\n",
            "Epochs :  856   Loss :  0.08449861407279968\n",
            "Epochs :  857   Loss :  0.08447489142417908\n",
            "Epochs :  858   Loss :  0.08445113897323608\n",
            "Epochs :  859   Loss :  0.08442746847867966\n",
            "Epochs :  860   Loss :  0.08440384268760681\n",
            "Epochs :  861   Loss :  0.08438017219305038\n",
            "Epochs :  862   Loss :  0.08435658365488052\n",
            "Epochs :  863   Loss :  0.08433301001787186\n",
            "Epochs :  864   Loss :  0.08430950343608856\n",
            "Epochs :  865   Loss :  0.08428599685430527\n",
            "Epochs :  866   Loss :  0.08426254242658615\n",
            "Epochs :  867   Loss :  0.08423905819654465\n",
            "Epochs :  868   Loss :  0.08421563357114792\n",
            "Epochs :  869   Loss :  0.08419226855039597\n",
            "Epochs :  870   Loss :  0.08416888862848282\n",
            "Epochs :  871   Loss :  0.08414549380540848\n",
            "Epochs :  872   Loss :  0.08412221074104309\n",
            "Epochs :  873   Loss :  0.0840989202260971\n",
            "Epochs :  874   Loss :  0.08407565951347351\n",
            "Epochs :  875   Loss :  0.08405241370201111\n",
            "Epochs :  876   Loss :  0.0840291902422905\n",
            "Epochs :  877   Loss :  0.08400598913431168\n",
            "Epochs :  878   Loss :  0.08398281782865524\n",
            "Epochs :  879   Loss :  0.08395969122648239\n",
            "Epochs :  880   Loss :  0.08393649011850357\n",
            "Epochs :  881   Loss :  0.08391334861516953\n",
            "Epochs :  882   Loss :  0.08389029651880264\n",
            "Epochs :  883   Loss :  0.08386719971895218\n",
            "Epochs :  884   Loss :  0.08384410291910172\n",
            "Epochs :  885   Loss :  0.08382104337215424\n",
            "Epochs :  886   Loss :  0.08379802107810974\n",
            "Epochs :  887   Loss :  0.08377502113580704\n",
            "Epochs :  888   Loss :  0.08375197649002075\n",
            "Epochs :  889   Loss :  0.08372905105352402\n",
            "Epochs :  890   Loss :  0.08370604366064072\n",
            "Epochs :  891   Loss :  0.0836830660700798\n",
            "Epochs :  892   Loss :  0.08366012573242188\n",
            "Epochs :  893   Loss :  0.08363719284534454\n",
            "Epochs :  894   Loss :  0.08361425250768661\n",
            "Epochs :  895   Loss :  0.08359136432409286\n",
            "Epochs :  896   Loss :  0.08356846123933792\n",
            "Epochs :  897   Loss :  0.08354556560516357\n",
            "Epochs :  898   Loss :  0.083522729575634\n",
            "Epochs :  899   Loss :  0.08349987119436264\n",
            "Epochs :  900   Loss :  0.08347702026367188\n",
            "Epochs :  901   Loss :  0.0834541767835617\n",
            "Epochs :  902   Loss :  0.08343132585287094\n",
            "Epochs :  903   Loss :  0.08340852707624435\n",
            "Epochs :  904   Loss :  0.08338567614555359\n",
            "Epochs :  905   Loss :  0.08336284756660461\n",
            "Epochs :  906   Loss :  0.08334003388881683\n",
            "Epochs :  907   Loss :  0.08331722021102905\n",
            "Epochs :  908   Loss :  0.08329445868730545\n",
            "Epochs :  909   Loss :  0.08327171951532364\n",
            "Epochs :  910   Loss :  0.08324900269508362\n",
            "Epochs :  911   Loss :  0.08322622627019882\n",
            "Epochs :  912   Loss :  0.08320342749357224\n",
            "Epochs :  913   Loss :  0.08318070322275162\n",
            "Epochs :  914   Loss :  0.0831579864025116\n",
            "Epochs :  915   Loss :  0.0831351950764656\n",
            "Epochs :  916   Loss :  0.08311246335506439\n",
            "Epochs :  917   Loss :  0.08308969438076019\n",
            "Epochs :  918   Loss :  0.08306695520877838\n",
            "Epochs :  919   Loss :  0.08304421603679657\n",
            "Epochs :  920   Loss :  0.08302149921655655\n",
            "Epochs :  921   Loss :  0.08299868553876877\n",
            "Epochs :  922   Loss :  0.08297593146562576\n",
            "Epochs :  923   Loss :  0.08295319229364395\n",
            "Epochs :  924   Loss :  0.08293046057224274\n",
            "Epochs :  925   Loss :  0.08290772885084152\n",
            "Epochs :  926   Loss :  0.08288496732711792\n",
            "Epochs :  927   Loss :  0.08286219835281372\n",
            "Epochs :  928   Loss :  0.08283950388431549\n",
            "Epochs :  929   Loss :  0.08281674981117249\n",
            "Epochs :  930   Loss :  0.08279401063919067\n",
            "Epochs :  931   Loss :  0.08277127146720886\n",
            "Epochs :  932   Loss :  0.08274850249290466\n",
            "Epochs :  933   Loss :  0.08272578567266464\n",
            "Epochs :  934   Loss :  0.08270306140184402\n",
            "Epochs :  935   Loss :  0.08268029242753983\n",
            "Epochs :  936   Loss :  0.08265751600265503\n",
            "Epochs :  937   Loss :  0.082634836435318\n",
            "Epochs :  938   Loss :  0.08261201530694962\n",
            "Epochs :  939   Loss :  0.082589291036129\n",
            "Epochs :  940   Loss :  0.08256655186414719\n",
            "Epochs :  941   Loss :  0.08254379779100418\n",
            "Epochs :  942   Loss :  0.08252105116844177\n",
            "Epochs :  943   Loss :  0.08249828219413757\n",
            "Epochs :  944   Loss :  0.08247551321983337\n",
            "Epochs :  945   Loss :  0.08245275914669037\n",
            "Epochs :  946   Loss :  0.08243001997470856\n",
            "Epochs :  947   Loss :  0.08240722864866257\n",
            "Epochs :  948   Loss :  0.08238440752029419\n",
            "Epochs :  949   Loss :  0.0823616310954094\n",
            "Epochs :  950   Loss :  0.08233886957168579\n",
            "Epochs :  951   Loss :  0.08231604844331741\n",
            "Epochs :  952   Loss :  0.08229321986436844\n",
            "Epochs :  953   Loss :  0.08227040618658066\n",
            "Epochs :  954   Loss :  0.08224757760763168\n",
            "Epochs :  955   Loss :  0.0822247564792633\n",
            "Epochs :  956   Loss :  0.08220186084508896\n",
            "Epochs :  957   Loss :  0.08217903971672058\n",
            "Epochs :  958   Loss :  0.08215618878602982\n",
            "Epochs :  959   Loss :  0.08213325589895248\n",
            "Epochs :  960   Loss :  0.08211038261651993\n",
            "Epochs :  961   Loss :  0.08208756893873215\n",
            "Epochs :  962   Loss :  0.08206452429294586\n",
            "Epochs :  963   Loss :  0.08204159140586853\n",
            "Epochs :  964   Loss :  0.08201868087053299\n",
            "Epochs :  965   Loss :  0.08199574053287506\n",
            "Epochs :  966   Loss :  0.08197274059057236\n",
            "Epochs :  967   Loss :  0.08194977790117264\n",
            "Epochs :  968   Loss :  0.08192677050828934\n",
            "Epochs :  969   Loss :  0.08190375566482544\n",
            "Epochs :  970   Loss :  0.08188075572252274\n",
            "Epochs :  971   Loss :  0.08185768872499466\n",
            "Epochs :  972   Loss :  0.08183466643095016\n",
            "Epochs :  973   Loss :  0.08181162178516388\n",
            "Epochs :  974   Loss :  0.0817885473370552\n",
            "Epochs :  975   Loss :  0.08176549524068832\n",
            "Epochs :  976   Loss :  0.08174237608909607\n",
            "Epochs :  977   Loss :  0.08171924203634262\n",
            "Epochs :  978   Loss :  0.08169615268707275\n",
            "Epochs :  979   Loss :  0.08167310059070587\n",
            "Epochs :  980   Loss :  0.081650011241436\n",
            "Epochs :  981   Loss :  0.0816270038485527\n",
            "Epochs :  982   Loss :  0.0816039964556694\n",
            "Epochs :  983   Loss :  0.08158095926046371\n",
            "Epochs :  984   Loss :  0.08155787736177444\n",
            "Epochs :  985   Loss :  0.08153480291366577\n",
            "Epochs :  986   Loss :  0.08151175826787949\n",
            "Epochs :  987   Loss :  0.08148865401744843\n",
            "Epochs :  988   Loss :  0.08146555721759796\n",
            "Epochs :  989   Loss :  0.08144243061542511\n",
            "Epochs :  990   Loss :  0.08141929656267166\n",
            "Epochs :  991   Loss :  0.0813961997628212\n",
            "Epochs :  992   Loss :  0.08137301355600357\n",
            "Epochs :  993   Loss :  0.08134985715150833\n",
            "Epochs :  994   Loss :  0.08132673799991608\n",
            "Epochs :  995   Loss :  0.08130355924367905\n",
            "Epochs :  996   Loss :  0.08128040283918381\n",
            "Epochs :  997   Loss :  0.08125726878643036\n",
            "Epochs :  998   Loss :  0.08123409003019333\n",
            "Epochs :  999   Loss :  0.08121092617511749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqzNKd8K9aDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}